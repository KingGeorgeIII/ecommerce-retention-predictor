{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Customer Retention\n",
    "\n",
    "This notebook creates features for predicting customer repurchase probability:\n",
    "- Customer behavioral features (RFM analysis)\n",
    "- Purchase history aggregations\n",
    "- Product preference features\n",
    "- Seasonality features\n",
    "- Target variable creation (will customer repurchase?)\n",
    "\n",
    "Processed features are saved to the processed data layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data from staging layer\n",
    "stage_data_path = '../data/stage/'\n",
    "\n",
    "customers_df = pd.read_csv(f'{stage_data_path}/cleaned_customers.csv')\n",
    "products_df = pd.read_csv(f'{stage_data_path}/cleaned_products.csv')\n",
    "transactions_df = pd.read_csv(f'{stage_data_path}/cleaned_transactions.csv')\n",
    "\n",
    "# Convert date columns\n",
    "customers_df['registration_date'] = pd.to_datetime(customers_df['registration_date'])\n",
    "products_df['launch_date'] = pd.to_datetime(products_df['launch_date'])\n",
    "transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'])\n",
    "\n",
    "print(\"=== CLEAN DATA LOADED ===\")\n",
    "print(f\"Customers: {len(customers_df):,} records\")\n",
    "print(f\"Products: {len(products_df):,} records\")\n",
    "print(f\"Transactions: {len(transactions_df):,} records\")\n",
    "\n",
    "# Set analysis date (latest transaction date)\n",
    "analysis_date = transactions_df['transaction_date'].max()\n",
    "print(f\"\\nAnalysis date: {analysis_date.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM Analysis (Recency, Frequency, Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM metrics for each customer\n",
    "def calculate_rfm(transactions_df, analysis_date):\n",
    "    # Filter completed transactions only\n",
    "    completed_transactions = transactions_df[transactions_df['order_status'] == 'completed'].copy()\n",
    "    \n",
    "    # Calculate RFM metrics\n",
    "    rfm = completed_transactions.groupby('customer_id').agg({\n",
    "        'transaction_date': lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "        'transaction_id': 'count',  # Frequency\n",
    "        'total_amount': 'sum'  # Monetary\n",
    "    }).round(2)\n",
    "    \n",
    "    rfm.columns = ['recency_days', 'frequency', 'monetary_value']\n",
    "    \n",
    "    # Calculate RFM scores (1-5 scale)\n",
    "    rfm['recency_score'] = pd.qcut(rfm['recency_days'], 5, labels=[5, 4, 3, 2, 1])\n",
    "    rfm['frequency_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])\n",
    "    rfm['monetary_score'] = pd.qcut(rfm['monetary_value'], 5, labels=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Convert scores to numeric\n",
    "    rfm['recency_score'] = rfm['recency_score'].astype(int)\n",
    "    rfm['frequency_score'] = rfm['frequency_score'].astype(int)\n",
    "    rfm['monetary_score'] = rfm['monetary_score'].astype(int)\n",
    "    \n",
    "    # Calculate overall RFM score\n",
    "    rfm['rfm_score'] = (rfm['recency_score'].astype(str) + \n",
    "                       rfm['frequency_score'].astype(str) + \n",
    "                       rfm['monetary_score'].astype(str))\n",
    "    \n",
    "    return rfm.reset_index()\n",
    "\n",
    "rfm_df = calculate_rfm(transactions_df, analysis_date)\n",
    "print(f\"RFM analysis completed for {len(rfm_df):,} customers\")\n",
    "\n",
    "# Display RFM distribution\n",
    "print(\"\\n=== RFM SCORE DISTRIBUTION ===\")\n",
    "print(f\"Recency Score: {rfm_df['recency_score'].describe()}\")\n",
    "print(f\"Frequency Score: {rfm_df['frequency_score'].describe()}\")\n",
    "print(f\"Monetary Score: {rfm_df['monetary_score'].describe()}\")\n",
    "\n",
    "rfm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Behavioral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional behavioral features\n",
    "def create_behavioral_features(transactions_df, customers_df, analysis_date):\n",
    "    # Filter completed transactions\n",
    "    completed_txns = transactions_df[transactions_df['order_status'] == 'completed'].copy()\n",
    "    \n",
    "    # Customer-level aggregations\n",
    "    customer_features = completed_txns.groupby('customer_id').agg({\n",
    "        'total_amount': ['sum', 'mean', 'std', 'count'],\n",
    "        'quantity': ['sum', 'mean'],\n",
    "        'discount_amount': ['sum', 'mean'],\n",
    "        'shipping_cost': ['sum', 'mean'],\n",
    "        'transaction_date': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    customer_features.columns = ['_'.join(col).strip() for col in customer_features.columns]\n",
    "    customer_features = customer_features.reset_index()\n",
    "    \n",
    "    # Calculate customer lifetime (days since first purchase)\n",
    "    customer_features['customer_lifetime_days'] = (\n",
    "        customer_features['transaction_date_max'] - customer_features['transaction_date_min']\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    # Calculate days since registration to first purchase\n",
    "    reg_dates = customers_df.set_index('customer_id')['registration_date'].to_dict()\n",
    "    customer_features['registration_date'] = customer_features['customer_id'].map(reg_dates)\n",
    "    customer_features['days_reg_to_first_purchase'] = (\n",
    "        customer_features['transaction_date_min'] - pd.to_datetime(customer_features['registration_date'])\n",
    "    ).dt.days\n",
    "    \n",
    "    # Calculate purchase frequency (purchases per day of lifetime)\n",
    "    customer_features['purchase_frequency'] = (\n",
    "        customer_features['total_amount_count'] / customer_features['customer_lifetime_days']\n",
    "    ).fillna(0).round(4)\n",
    "    \n",
    "    # Fill NaN values in std with 0 (customers with only 1 purchase)\n",
    "    customer_features['total_amount_std'] = customer_features['total_amount_std'].fillna(0)\n",
    "    \n",
    "    # Calculate days since last purchase\n",
    "    customer_features['days_since_last_purchase'] = (\n",
    "        analysis_date - customer_features['transaction_date_max']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    customer_features = customer_features.drop(['transaction_date_min', 'transaction_date_max', 'registration_date'], axis=1)\n",
    "    \n",
    "    return customer_features\n",
    "\n",
    "behavioral_features = create_behavioral_features(transactions_df, customers_df, analysis_date)\n",
    "print(f\"Behavioral features created for {len(behavioral_features):,} customers\")\n",
    "\n",
    "behavioral_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Preference Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create product preference features\n",
    "def create_product_features(transactions_df, products_df):\n",
    "    # Merge transactions with product info\n",
    "    txn_products = transactions_df.merge(products_df[['product_id', 'category', 'price', 'rating']], on='product_id')\n",
    "    \n",
    "    # Filter completed transactions\n",
    "    completed_txns = txn_products[txn_products['order_status'] == 'completed'].copy()\n",
    "    \n",
    "    # Category preferences\n",
    "    category_counts = completed_txns.groupby(['customer_id', 'category']).size().unstack(fill_value=0)\n",
    "    category_counts.columns = [f'purchases_{col.lower()}' for col in category_counts.columns]\n",
    "    \n",
    "    # Calculate total purchases per customer\n",
    "    category_counts['total_purchases'] = category_counts.sum(axis=1)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    for col in category_counts.columns[:-1]:  # Exclude total_purchases\n",
    "        category_counts[f'pct_{col}'] = (\n",
    "            category_counts[col] / category_counts['total_purchases'] * 100\n",
    "        ).round(2)\n",
    "    \n",
    "    # Find preferred category\n",
    "    pct_cols = [col for col in category_counts.columns if col.startswith('pct_')]\n",
    "    category_counts['preferred_category'] = category_counts[pct_cols].idxmax(axis=1)\n",
    "    category_counts['preferred_category'] = category_counts['preferred_category'].str.replace('pct_purchases_', '')\n",
    "    \n",
    "    # Calculate average product metrics\n",
    "    product_metrics = completed_txns.groupby('customer_id').agg({\n",
    "        'price': 'mean',\n",
    "        'rating': 'mean'\n",
    "    }).round(2)\n",
    "    product_metrics.columns = ['avg_product_price', 'avg_product_rating']\n",
    "    \n",
    "    # Merge category and product metrics\n",
    "    product_features = category_counts.join(product_metrics)\n",
    "    \n",
    "    return product_features.reset_index()\n",
    "\n",
    "product_features = create_product_features(transactions_df, products_df)\n",
    "print(f\"Product preference features created for {len(product_features):,} customers\")\n",
    "\n",
    "product_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality and Time-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seasonality features\n",
    "def create_seasonality_features(transactions_df):\n",
    "    completed_txns = transactions_df[transactions_df['order_status'] == 'completed'].copy()\n",
    "    \n",
    "    # Extract time components\n",
    "    completed_txns['month'] = completed_txns['transaction_date'].dt.month\n",
    "    completed_txns['day_of_week'] = completed_txns['transaction_date'].dt.dayofweek\n",
    "    completed_txns['quarter'] = completed_txns['transaction_date'].dt.quarter\n",
    "    completed_txns['is_weekend'] = completed_txns['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Define seasons\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'summer'\n",
    "        else:\n",
    "            return 'fall'\n",
    "    \n",
    "    completed_txns['season'] = completed_txns['month'].apply(get_season)\n",
    "    \n",
    "    # Customer seasonality features\n",
    "    seasonality_features = completed_txns.groupby('customer_id').agg({\n",
    "        'month': lambda x: x.mode().iloc[0] if not x.empty else 1,  # Most common month\n",
    "        'day_of_week': lambda x: x.mode().iloc[0] if not x.empty else 1,  # Most common day\n",
    "        'quarter': lambda x: x.mode().iloc[0] if not x.empty else 1,  # Most common quarter\n",
    "        'is_weekend': 'mean',  # Percentage of weekend purchases\n",
    "        'season': lambda x: x.mode().iloc[0] if not x.empty else 'spring'  # Most common season\n",
    "    }).round(2)\n",
    "    \n",
    "    seasonality_features.columns = [\n",
    "        'preferred_month', 'preferred_day_of_week', 'preferred_quarter', \n",
    "        'weekend_purchase_rate', 'preferred_season'\n",
    "    ]\n",
    "    \n",
    "    return seasonality_features.reset_index()\n",
    "\n",
    "seasonality_features = create_seasonality_features(transactions_df)\n",
    "print(f\"Seasonality features created for {len(seasonality_features):,} customers\")\n",
    "\n",
    "seasonality_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Target Variable (Repurchase Probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable for repurchase prediction\n",
    "def create_target_variable(transactions_df, analysis_date, prediction_window_days=90):\n",
    "    \"\"\"\n",
    "    Target: Will customer make a purchase in the next X days?\n",
    "    For simulation, we'll use a rule-based approach based on customer behavior\n",
    "    \"\"\"\n",
    "    completed_txns = transactions_df[transactions_df['order_status'] == 'completed'].copy()\n",
    "    \n",
    "    # Calculate customer metrics for target generation\n",
    "    customer_metrics = completed_txns.groupby('customer_id').agg({\n",
    "        'transaction_date': ['min', 'max', 'count'],\n",
    "        'total_amount': ['sum', 'mean']\n",
    "    })\n",
    "    \n",
    "    customer_metrics.columns = ['first_purchase', 'last_purchase', 'purchase_count', 'total_spent', 'avg_order_value']\n",
    "    customer_metrics = customer_metrics.reset_index()\n",
    "    \n",
    "    # Days since last purchase\n",
    "    customer_metrics['days_since_last'] = (analysis_date - customer_metrics['last_purchase']).dt.days\n",
    "    \n",
    "    # Customer lifetime in days\n",
    "    customer_metrics['customer_lifetime'] = (customer_metrics['last_purchase'] - customer_metrics['first_purchase']).dt.days + 1\n",
    "    \n",
    "    # Average days between purchases\n",
    "    customer_metrics['avg_days_between_purchases'] = (\n",
    "        customer_metrics['customer_lifetime'] / customer_metrics['purchase_count']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Rule-based target generation (simulate reality)\n",
    "    def predict_repurchase(row):\n",
    "        # High probability conditions\n",
    "        if (row['days_since_last'] <= 30 and row['purchase_count'] >= 5) or \\\n",
    "           (row['avg_order_value'] >= 100 and row['days_since_last'] <= 60) or \\\n",
    "           (row['purchase_count'] >= 10 and row['days_since_last'] <= 90):\n",
    "            return np.random.choice([0, 1], p=[0.2, 0.8])  # 80% chance\n",
    "        \n",
    "        # Medium probability conditions\n",
    "        elif (row['days_since_last'] <= 60 and row['purchase_count'] >= 3) or \\\n",
    "             (row['avg_order_value'] >= 50 and row['days_since_last'] <= 120):\n",
    "            return np.random.choice([0, 1], p=[0.5, 0.5])  # 50% chance\n",
    "        \n",
    "        # Low probability conditions\n",
    "        elif row['days_since_last'] <= 180 and row['purchase_count'] >= 2:\n",
    "            return np.random.choice([0, 1], p=[0.7, 0.3])  # 30% chance\n",
    "        \n",
    "        # Very low probability\n",
    "        else:\n",
    "            return np.random.choice([0, 1], p=[0.9, 0.1])  # 10% chance\n",
    "    \n",
    "    # Set seed for reproducible target generation\n",
    "    np.random.seed(42)\n",
    "    customer_metrics['will_repurchase'] = customer_metrics.apply(predict_repurchase, axis=1)\n",
    "    \n",
    "    return customer_metrics[['customer_id', 'will_repurchase']]\n",
    "\n",
    "target_df = create_target_variable(transactions_df, analysis_date)\n",
    "print(f\"Target variable created for {len(target_df):,} customers\")\n",
    "\n",
    "# Display target distribution\n",
    "target_dist = target_df['will_repurchase'].value_counts()\n",
    "print(f\"\\n=== TARGET DISTRIBUTION ===\")\n",
    "print(f\"Will NOT repurchase (0): {target_dist[0]:,} ({target_dist[0]/len(target_df)*100:.1f}%)\")\n",
    "print(f\"Will repurchase (1): {target_dist[1]:,} ({target_dist[1]/len(target_df)*100:.1f}%)\")\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all feature sets\n",
    "print(\"=== MERGING ALL FEATURES ===\")\n",
    "\n",
    "# Start with customer basic info\n",
    "customer_basic = customers_df[['customer_id', 'age', 'gender', 'preferred_category']].copy()\n",
    "\n",
    "# Merge all feature sets\n",
    "feature_sets = [\n",
    "    (rfm_df, 'RFM'),\n",
    "    (behavioral_features, 'Behavioral'),\n",
    "    (product_features, 'Product Preferences'),\n",
    "    (seasonality_features, 'Seasonality'),\n",
    "    (target_df, 'Target')\n",
    "]\n",
    "\n",
    "final_dataset = customer_basic.copy()\n",
    "\n",
    "for df, name in feature_sets:\n",
    "    initial_count = len(final_dataset)\n",
    "    final_dataset = final_dataset.merge(df, on='customer_id', how='inner')\n",
    "    print(f\"After merging {name}: {len(final_dataset):,} customers (lost {initial_count - len(final_dataset)})\")\n",
    "\n",
    "print(f\"\\nâœ… Final dataset created with {len(final_dataset):,} customers and {len(final_dataset.columns)} features\")\n",
    "\n",
    "# Display feature summary\n",
    "print(f\"\\n=== FEATURE SUMMARY ===\")\n",
    "print(f\"Total features: {len(final_dataset.columns)}\")\n",
    "print(f\"Features: {list(final_dataset.columns)}\")\n",
    "\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "final_dataset_encoded = final_dataset.copy()\n",
    "\n",
    "# Label encode categorical variables\n",
    "categorical_cols = ['gender', 'preferred_category', 'preferred_season']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in final_dataset_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        final_dataset_encoded[f'{col}_encoded'] = le.fit_transform(final_dataset_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# One-hot encode some categorical variables for better model performance\n",
    "# Gender\n",
    "if 'gender' in final_dataset_encoded.columns:\n",
    "    gender_dummies = pd.get_dummies(final_dataset_encoded['gender'], prefix='gender')\n",
    "    final_dataset_encoded = pd.concat([final_dataset_encoded, gender_dummies], axis=1)\n",
    "\n",
    "# Preferred category\n",
    "if 'preferred_category' in final_dataset_encoded.columns:\n",
    "    category_dummies = pd.get_dummies(final_dataset_encoded['preferred_category'], prefix='prefers')\n",
    "    final_dataset_encoded = pd.concat([final_dataset_encoded, category_dummies], axis=1)\n",
    "\n",
    "# Preferred season\n",
    "if 'preferred_season' in final_dataset_encoded.columns:\n",
    "    season_dummies = pd.get_dummies(final_dataset_encoded['preferred_season'], prefix='season')\n",
    "    final_dataset_encoded = pd.concat([final_dataset_encoded, season_dummies], axis=1)\n",
    "\n",
    "print(f\"\\nâœ… Categorical encoding completed. Dataset now has {len(final_dataset_encoded.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature correlations with target variable\n",
    "numeric_cols = final_dataset_encoded.select_dtypes(include=[np.number]).columns\n",
    "correlation_with_target = final_dataset_encoded[numeric_cols].corr()['will_repurchase'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"=== TOP FEATURES CORRELATED WITH REPURCHASE ===\")\n",
    "print(correlation_with_target.head(15))\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = correlation_with_target.drop('will_repurchase').head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features.values]\n",
    "plt.barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Correlation with Repurchase Target')\n",
    "plt.title('Top 15 Features Correlated with Customer Repurchase')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Final Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final training dataset\n",
    "# Remove non-numeric and redundant columns\n",
    "columns_to_drop = [\n",
    "    'customer_id',  # Identifier, not a feature\n",
    "    'gender', 'preferred_category', 'preferred_season',  # Original categorical (we have encoded versions)\n",
    "    'rfm_score'  # String combination of scores (we have individual scores)\n",
    "]\n",
    "\n",
    "# Drop columns that exist in the dataset\n",
    "columns_to_drop = [col for col in columns_to_drop if col in final_dataset_encoded.columns]\n",
    "training_data = final_dataset_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "# Separate features and target\n",
    "X = training_data.drop('will_repurchase', axis=1)\n",
    "y = training_data['will_repurchase']\n",
    "\n",
    "print(f\"=== FINAL TRAINING DATASET ===\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nFeature list: {list(X.columns)}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_values = X.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(f\"\\nâš ï¸ Missing values found:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    # Fill missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    print(\"âœ… Missing values filled with mean\")\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values found\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "processed_data_path = '../data/processed/'\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# Save feature engineered dataset\n",
    "final_dataset_encoded.to_csv(f'{processed_data_path}/customer_features.csv', index=False)\n",
    "\n",
    "# Save training data\n",
    "training_data.to_csv(f'{processed_data_path}/training_data.csv', index=False)\n",
    "\n",
    "# Save feature matrix and target separately for ML\n",
    "X.to_csv(f'{processed_data_path}/X_features.csv', index=False)\n",
    "y.to_csv(f'{processed_data_path}/y_target.csv', index=False)\n",
    "\n",
    "# Save label encoders for future use\n",
    "import pickle\n",
    "with open(f'{processed_data_path}/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Save feature engineering summary\n",
    "feature_summary = {\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'total_customers': len(final_dataset_encoded),\n",
    "    'total_features': len(X.columns),\n",
    "    'target_distribution': y.value_counts().to_dict(),\n",
    "    'feature_list': list(X.columns),\n",
    "    'categorical_encodings': {col: dict(zip(le.classes_, le.transform(le.classes_))) \n",
    "                             for col, le in label_encoders.items()}\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{processed_data_path}/feature_engineering_summary.json', 'w') as f:\n",
    "    json.dump(feature_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ… Processed data successfully saved to processed data layer:\")\n",
    "print(f\"   - customer_features.csv: {len(final_dataset_encoded):,} records, {len(final_dataset_encoded.columns)} features\")\n",
    "print(f\"   - training_data.csv: {len(training_data):,} records\")\n",
    "print(f\"   - X_features.csv: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "print(f\"   - y_target.csv: {len(y):,} samples\")\n",
    "print(f\"   - label_encoders.pkl: {len(label_encoders)} encoders\")\n",
    "print(f\"   - feature_engineering_summary.json: metadata\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Data is now ready for machine learning model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}