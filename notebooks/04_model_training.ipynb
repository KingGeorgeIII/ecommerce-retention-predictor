{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Deep Learning Model for Customer Retention\n",
    "\n",
    "This notebook creates and trains a deep learning model to predict customer repurchase probability:\n",
    "- Load processed features\n",
    "- Data preprocessing and scaling\n",
    "- Neural network architecture design\n",
    "- Model training with validation\n",
    "- Model evaluation and metrics\n",
    "- Save trained model\n",
    "\n",
    "The trained model is saved for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# Load feature matrix and target\n",
    "X = pd.read_csv(f'{processed_data_path}/X_features.csv')\n",
    "y = pd.read_csv(f'{processed_data_path}/y_target.csv')['will_repurchase']\n",
    "\n",
    "# Load feature engineering summary\n",
    "with open(f'{processed_data_path}/feature_engineering_summary.json', 'r') as f:\n",
    "    feature_summary = json.load(f)\n",
    "\n",
    "print(\"=== PROCESSED DATA LOADED ===\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "print(f\"Features: {len(X.columns)} total\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\n=== FEATURE STATISTICS ===\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"=== DATA SPLIT ===\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Check target distribution in each set\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"Train: {y_train.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Val: {y_val.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts(normalize=True).round(3).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"=== FEATURE SCALING ===\")\n",
    "print(f\"Original feature range: [{X_train.min().min():.3f}, {X_train.max().max():.3f}]\")\n",
    "print(f\"Scaled feature range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n",
    "print(f\"Scaled mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Scaled std: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "X_val_scaled = X_val_scaled.astype(np.float32)\n",
    "X_test_scaled = X_test_scaled.astype(np.float32)\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_val = y_val.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "print(\"=== CLASS IMBALANCE HANDLING ===\")\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# Calculate positive/negative ratios\n",
    "neg_count = np.sum(y_train == 0)\n",
    "pos_count = np.sum(y_train == 1)\n",
    "total = len(y_train)\n",
    "\n",
    "print(f\"Negative samples: {neg_count:,} ({neg_count/total*100:.1f}%)\")\n",
    "print(f\"Positive samples: {pos_count:,} ({pos_count/total*100:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {neg_count/pos_count:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network model\n",
    "def create_retention_model(input_dim, dropout_rate=0.3, l2_reg=0.001):\n",
    "    \"\"\"\n",
    "    Create a deep neural network for customer retention prediction\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Dense(256, activation='relu', input_shape=(input_dim,),\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                    name='dense_1'),\n",
    "        layers.BatchNormalization(name='batch_norm_1'),\n",
    "        layers.Dropout(dropout_rate, name='dropout_1'),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        layers.Dense(128, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                    name='dense_2'),\n",
    "        layers.BatchNormalization(name='batch_norm_2'),\n",
    "        layers.Dropout(dropout_rate, name='dropout_2'),\n",
    "        \n",
    "        # Hidden layer 3\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                    name='dense_3'),\n",
    "        layers.BatchNormalization(name='batch_norm_3'),\n",
    "        layers.Dropout(dropout_rate/2, name='dropout_3'),\n",
    "        \n",
    "        # Hidden layer 4\n",
    "        layers.Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                    name='dense_4'),\n",
    "        layers.Dropout(dropout_rate/2, name='dropout_4'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_dimension = X_train_scaled.shape[1]\n",
    "model = create_retention_model(input_dimension)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"=== NEURAL NETWORK ARCHITECTURE ===\")\n",
    "model.summary()\n",
    "\n",
    "# Visualize model architecture\n",
    "keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        '../models/best_retention_model.h5',\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"=== STARTING MODEL TRAINING ===\")\n",
    "print(f\"Training samples: {len(X_train_scaled):,}\")\n",
    "print(f\"Validation samples: {len(X_val_scaled):,}\")\n",
    "print(f\"Features: {input_dimension}\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Model Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Model Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # AUC\n",
    "    axes[1, 0].plot(history.history['auc'], label='Training AUC')\n",
    "    axes[1, 0].plot(history.history['val_auc'], label='Validation AUC')\n",
    "    axes[1, 0].set_title('Model AUC')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('AUC')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning Rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        axes[1, 1].plot(history.history['lr'], label='Learning Rate')\n",
    "        axes[1, 1].set_title('Learning Rate Schedule')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        # Precision-Recall if LR not available\n",
    "        axes[1, 1].plot(history.history['precision'], label='Training Precision')\n",
    "        axes[1, 1].plot(history.history['val_precision'], label='Validation Precision')\n",
    "        axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
    "        axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "        axes[1, 1].set_title('Precision & Recall')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Score')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# Print training summary\n",
    "best_epoch = np.argmax(history.history['val_auc'])\n",
    "print(f\"\\n=== TRAINING SUMMARY ===\")\n",
    "print(f\"Best epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation AUC: {max(history.history['val_auc']):.4f}\")\n",
    "print(f\"Best validation accuracy: {history.history['val_accuracy'][best_epoch]:.4f}\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "print(\"=== MODEL EVALUATION ON TEST SET ===\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Will Not Repurchase', 'Will Repurchase']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Will Not Repurchase', 'Will Repurchase'],\n",
    "            yticklabels=['Will Not Repurchase', 'Will Repurchase'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {test_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Prediction distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_pred_proba[y_test == 0], alpha=0.7, label='Will Not Repurchase', bins=30, density=True)\n",
    "plt.hist(y_pred_proba[y_test == 1], alpha=0.7, label='Will Repurchase', bins=30, density=True)\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Prediction Probability Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation feature importance (simplified approach)\n",
    "def calculate_feature_importance(model, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using permutation method\n",
    "    \"\"\"\n",
    "    baseline_score = roc_auc_score(y_test, model.predict(X_test))\n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X_test.shape[1]):\n",
    "        # Create copy and shuffle one feature\n",
    "        X_permuted = X_test.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        # Calculate score with permuted feature\n",
    "        permuted_score = roc_auc_score(y_test, model.predict(X_permuted))\n",
    "        \n",
    "        # Importance is the decrease in performance\n",
    "        importance = baseline_score - permuted_score\n",
    "        importances.append(importance)\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Calculate feature importance (on a sample for speed)\n",
    "sample_size = min(1000, len(X_test_scaled))\n",
    "sample_indices = np.random.choice(len(X_test_scaled), sample_size, replace=False)\n",
    "X_sample = X_test_scaled[sample_indices]\n",
    "y_sample = y_test[sample_indices]\n",
    "\n",
    "feature_importance = calculate_feature_importance(model, X_sample, y_sample, X.columns)\n",
    "\n",
    "# Plot top feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features['importance']]\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance (AUC decrease when permuted)')\n",
    "plt.title('Top 15 Most Important Features for Customer Retention Prediction')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== TOP 10 MOST IMPORTANT FEATURES ===\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation and Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "print(\"=== BUSINESS INSIGHTS FROM MODEL ===\")\n",
    "print(\"\\nðŸŽ¯ KEY RETENTION DRIVERS:\")\n",
    "\n",
    "top_5_features = feature_importance.head(5)\n",
    "for idx, row in top_5_features.iterrows():\n",
    "    feature = row['feature']\n",
    "    importance = row['importance']\n",
    "    \n",
    "    if 'recency' in feature.lower():\n",
    "        print(f\"â€¢ {feature}: Recent customer activity is crucial for retention\")\n",
    "    elif 'frequency' in feature.lower():\n",
    "        print(f\"â€¢ {feature}: Purchase frequency strongly indicates future behavior\")\n",
    "    elif 'monetary' in feature.lower():\n",
    "        print(f\"â€¢ {feature}: Customer spending level is a key retention factor\")\n",
    "    elif 'total_amount' in feature.lower():\n",
    "        print(f\"â€¢ {feature}: Historical spend directly correlates with retention\")\n",
    "    elif 'age' in feature.lower():\n",
    "        print(f\"â€¢ {feature}: Customer demographics influence repurchase behavior\")\n",
    "    elif 'days_since' in feature.lower():\n",
    "        print(f\"â€¢ {feature}: Time since last interaction is critical\")\n",
    "    else:\n",
    "        print(f\"â€¢ {feature}: Important predictor (importance: {importance:.4f})\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\nðŸ“Š MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"â€¢ Test AUC: {test_auc:.3f} - {'Excellent' if test_auc > 0.9 else 'Good' if test_auc > 0.8 else 'Fair' if test_auc > 0.7 else 'Needs Improvement'}\")\n",
    "print(f\"â€¢ Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"â€¢ The model can effectively distinguish between customers who will and won't repurchase\")\n",
    "\n",
    "# Prediction examples\n",
    "print(f\"\\nðŸ” PREDICTION EXAMPLES:\")\n",
    "sample_predictions = y_pred_proba[:10].flatten()\n",
    "sample_actuals = y_test[:10]\n",
    "\n",
    "for i, (prob, actual) in enumerate(zip(sample_predictions, sample_actuals)):\n",
    "    risk_level = \"High\" if prob > 0.7 else \"Medium\" if prob > 0.3 else \"Low\"\n",
    "    actual_text = \"Repurchased\" if actual == 1 else \"Did not repurchase\"\n",
    "    print(f\"Customer {i+1}: {prob:.3f} retention probability ({risk_level} retention risk) - {actual_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_path = '../models/'\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(f'{models_path}/retention_model.h5')\n",
    "print(f\"âœ… Model saved to {models_path}/retention_model.h5\")\n",
    "\n",
    "# Save the scaler\n",
    "with open(f'{models_path}/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"âœ… Scaler saved to {models_path}/scaler.pkl\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(f'{models_path}/feature_importance.csv', index=False)\n",
    "print(f\"âœ… Feature importance saved to {models_path}/feature_importance.csv\")\n",
    "\n",
    "# Save model metrics and metadata\n",
    "model_metadata = {\n",
    "    'model_created': datetime.now().isoformat(),\n",
    "    'model_type': 'TensorFlow Neural Network',\n",
    "    'input_features': len(X.columns),\n",
    "    'training_samples': len(X_train_scaled),\n",
    "    'validation_samples': len(X_val_scaled),\n",
    "    'test_samples': len(X_test_scaled),\n",
    "    'architecture': {\n",
    "        'layers': [\n",
    "            {'type': 'Dense', 'units': 256, 'activation': 'relu'},\n",
    "            {'type': 'Dense', 'units': 128, 'activation': 'relu'},\n",
    "            {'type': 'Dense', 'units': 64, 'activation': 'relu'},\n",
    "            {'type': 'Dense', 'units': 32, 'activation': 'relu'},\n",
    "            {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'}\n",
    "        ],\n",
    "        'dropout': 0.3,\n",
    "        'l2_regularization': 0.001\n",
    "    },\n",
    "    'training_config': {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'best_epoch': int(best_epoch + 1),\n",
    "        'early_stopping': True,\n",
    "        'class_weights_used': True\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'test_auc': float(test_auc),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'best_val_auc': float(max(history.history['val_auc'])),\n",
    "        'best_val_accuracy': float(history.history['val_accuracy'][best_epoch])\n",
    "    },\n",
    "    'feature_names': list(X.columns),\n",
    "    'target_classes': ['will_not_repurchase', 'will_repurchase'],\n",
    "    'class_distribution': {\n",
    "        'train': y_train.tolist().count(1) / len(y_train),\n",
    "        'test': y_test.tolist().count(1) / len(y_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{models_path}/model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model metadata saved to {models_path}/model_metadata.json\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f'{models_path}/training_history.csv', index=False)\n",
    "print(f\"âœ… Training history saved to {models_path}/training_history.csv\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Model training and evaluation completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved to: {models_path}\")\n",
    "print(f\"ðŸš€ Model is ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the trained model for new predictions\n",
    "print(\"=== MODEL USAGE EXAMPLE ===\")\n",
    "print(\"\\n# Load the trained model and scaler:\")\n",
    "print(\"import tensorflow as tf\")\n",
    "print(\"import pickle\")\n",
    "print(\"import numpy as np\")\n",
    "print(\"\")\n",
    "print(\"# Load model and scaler\")\n",
    "print(\"model = tf.keras.models.load_model('../models/retention_model.h5')\")\n",
    "print(\"with open('../models/scaler.pkl', 'rb') as f:\")\n",
    "print(\"    scaler = pickle.load(f)\")\n",
    "print(\"\")\n",
    "print(\"# For new customer data:\")\n",
    "print(\"# 1. Ensure features are in the same order as training\")\n",
    "print(\"# 2. Scale features using the saved scaler\")\n",
    "print(\"# 3. Make prediction\")\n",
    "print(\"\")\n",
    "print(\"new_customer_features = scaler.transform(new_customer_data)\")\n",
    "print(\"retention_probability = model.predict(new_customer_features)[0][0]\")\n",
    "print(\"print(f'Retention probability: {retention_probability:.3f}')\")\n",
    "\n",
    "# Show actual example with test data\n",
    "example_customer = X_test_scaled[0:1]  # First test customer\n",
    "example_prediction = model.predict(example_customer)[0][0]\n",
    "example_actual = y_test[0]\n",
    "\n",
    "print(f\"\\nðŸ“ ACTUAL EXAMPLE:\")\n",
    "print(f\"Customer retention probability: {example_prediction:.3f}\")\n",
    "print(f\"Actual outcome: {'Repurchased' if example_actual == 1 else 'Did not repurchase'}\")\n",
    "print(f\"Model prediction: {'Will repurchase' if example_prediction > 0.5 else 'Will not repurchase'}\")\n",
    "print(f\"Prediction confidence: {max(example_prediction, 1-example_prediction):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}